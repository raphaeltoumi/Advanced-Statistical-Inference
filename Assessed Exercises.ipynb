{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Inference\n",
    "### Rapha√´l Toumi's Notebook - 07/06/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">\n",
    "1.  Download and import the Santander dataset. The labels of the test data are not publicly available,\n",
    "so create your own test set by randomly choosing half of the instances in the original training set.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math as mt\n",
    "from numpy.linalg import inv,pinv\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "base = '/Users/raphaeltoumi/Documents/Cours Eurecom S1/ASI/Assessed Exercise/santander-customer-transaction-prediction/'\n",
    "santanderDF = pd.read_csv(base + 'train.csv')\n",
    "\n",
    "#Create training and test set by randomly splitting in half the original dataset\n",
    "msk = np.random.rand(len(santanderDF)) < 0.5\n",
    "testDF = santanderDF[~msk]\n",
    "trainDF = santanderDF[msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">2. Comment on the distribution of class labels and the dimensionality of the input and how these may\n",
    "affect the analysis. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99914, 202)\n"
     ]
    }
   ],
   "source": [
    "#Print shape of training set\n",
    "print(trainDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we have in hand contains 200 numeric feature variables, a binary target label and an ID to identify any of the 100174 instances. Since we have a high number of features, we might want to perform some feature selection in order to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of instances having 1 for  label : 0.10040634946053606\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of instances having 1 for  label : '+str(sum(trainDF['target'])/len(trainDF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is imbalanced : there is a ratio of about 1 to 10 between the instances having 1 for label and the number of instances having 0. We will have to take this into account when evaluating the performance of our model, because even a dummy model that always output 0 will have around 90% accuracy. A better metric would be AUC (Area Under the ROC Curve), but we can also look at the confusion matrix to better explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">\n",
    "    a) Implement Bayesian linear regression.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To implement the Bayesian linear regression, we will use a Gaussian prior $P(ùê∞)=N(0,S)$ .\n",
    "Because they are conjugate, the likelihood and the posterior density are also Gaussian : \n",
    "\n",
    "$$P(\\mathbf{t}   |   \\: \\mathbf{w} , \\: \\mathbf{X},\\: \\sigma^2 ) =  {N}(\\mathbf{X}\\mathbf{w},\\: \\sigma^2\\mathbf{I})$$\n",
    "\n",
    "This formula tells that our observations can be expressed as a linear combination of the features added to a Gaussian noise.\n",
    "Because it is conjugate, posterior must be Gaussian with unknown parameters:\n",
    "\n",
    "$$P(\\mathbf{w}   |   \\: \\mathbf{X} , \\: \\mathbf{t},\\: \\sigma^2 ) =  N(\\: \\mu,\\: \\Sigma)$$\n",
    "\n",
    "Our goal is to estimate the parameters $\\mu$ and $\\Sigma$ of the posterior.\n",
    "\n",
    "Ignoring constant, the posterior is:\n",
    "\n",
    "$$P(\\mathbf{w}   | \\:   \\mathbf{X} ,\\: \\mathbf{t},\\: \\sigma^2 ) \\propto  \\exp(\\:-\\frac{1}{2}(\\mathbf{w}^T\\Sigma^{-1}\\mathbf{w}\\:-\\: 2\\mathbf{w}^T\\Sigma^{-1}\\mu)) $$\n",
    "Where the covariance matrix is: $\\Sigma=(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X}\\: +\\: \\mathbf{S}^{-1})^{-1}$ and the mean is: $\\mu=\\frac{1}{\\sigma^2}\\Sigma\\mathbf{X}^Tt$\n",
    "\n",
    "Then we can compute the predictive density:\n",
    "\n",
    "$$P(\\mathbf{t}_{new}|\\: \\mathbf{X}, \\: \\mathbf{t}, \\: \\mathbf{x}_{new},\\:  \\sigma^2)=N(\\mathbf{x}_{new}^T\\mu, \\: \\sigma^2 \\: +\\:  \\mathbf{x}_{new}^T\\Sigma^{-1}\\mathbf{x}_{new}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data, t, data_test):\n",
    "    \n",
    "    def weights(X,t):\n",
    "        return np.array((X.T.dot(X)).I.dot(X.T).dot(t))[0]\n",
    "\n",
    "    def variance(X, t, w):\n",
    "        tmp = t - X.dot(w)\n",
    "        return (tmp.dot(tmp.T)/X.shape[0])[0]\n",
    "\n",
    "    def covariance(X, S, sigma_2):\n",
    "        return inv(X.T.dot(X) / sigma_2 + inv(S))\n",
    "\n",
    "    def mean(X, t, cov, sigma_2):\n",
    "        return cov.dot(X.T).dot(t) / sigma_2\n",
    "    \n",
    "    X = np.matrix(data.copy())\n",
    "    W = weights(X,t)\n",
    "    print('weights done')\n",
    "    sigma_2 = variance(X, t, W)\n",
    "    print('variance done')\n",
    "    S = 10*np.identity(X.shape[1])\n",
    "    cov = covariance(X, S, sigma_2)\n",
    "    print('covar done')\n",
    "    mean =  mean(X, t, cov, sigma_2)\n",
    "    print('mean done')\n",
    "    # predictions\n",
    "    X_test = np.matrix(data_test.copy())\n",
    "    predictions = X_test.dot(mean.T)\n",
    "    del X,W,S,mean\n",
    "    variances = sigma_2 + X_test.dot(inv(cov)).dot(X_test.T)\n",
    "    print('variances done')\n",
    "    return predictions, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_S(X):\n",
    "    prior_cov_size = X.shape[1]\n",
    "    prior_cov = np.zeros((prior_cov_size,prior_cov_size)) + np.diag(np.random.uniform(0, 1, prior_cov_size))\n",
    "    return (prior_cov)\n",
    "\n",
    "def get_covariance_sigma(X, S, sigma_2):\n",
    "    return np.linalg.inv(X.T.dot(X) / sigma_2 + np.linalg.inv(S))\n",
    "\n",
    "def get_mean(X, t, covariance_sigma, variance):\n",
    "    return covariance_sigma.dot(X.T).dot(t) / variance\n",
    "\n",
    "def variance(X, t, w):\n",
    "        tmp = t - X.dot(w)\n",
    "        return (tmp.dot(tmp.T)/X.shape[0])[0]\n",
    "    \n",
    "def weights(X,t):\n",
    "        return np.array((X.T.dot(X)).I.dot(X.T).dot(t))[0]\n",
    "\n",
    "def bayesian_linear_regression(x, x_test, t, variance, degree):\n",
    "    X = get_nth_order(x, degree)\n",
    "    X_test = get_nth_order(x_test, degree)\n",
    "    \n",
    "    S = get_covariance_S(X)\n",
    "    W = weights(np.matrix(X),t)\n",
    "    \n",
    "    covariance_sigma = get_covariance_sigma(X, S, variance)\n",
    "    mu = get_mean(X, t, covariance_sigma, variance)\n",
    "    \n",
    "    pred_mean = X_test.dot(mu)\n",
    "    pred_var = (variance + X_test.dot(np.linalg.inv(covariance_sigma)).dot(X_test.T)).diagonal()\n",
    "        \n",
    "    return pred_mean, pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_lin_reg(X_train,X_test,t,order):\n",
    "    # add bias to the train dataset\n",
    "    X = np.append(X_train,np.ones((X_train.shape[0],1)),axis=1)\n",
    "\n",
    "    # compute w_hat and sigma2_hat (by maximizing likelihood)\n",
    "    w_hat = np.dot(np.dot(pinv(np.dot(X.T,X)),X.T),t)\n",
    "    sigma2_hat = np.dot((t - np.dot(X,w_hat)).T,(t - np.dot(X,w_hat)))/X_train.shape[0]\n",
    "\n",
    "    # add bias to test dataset\n",
    "    X_new = np.append(X_test,np.ones((X_test.shape[0],1)),axis=1)\n",
    "    \n",
    "    # covariance of the prior\n",
    "    S = 10 * np.identity(X.shape[1])\n",
    "    \n",
    "    # covariance of the posterior\n",
    "    SIGMA = inv(1/sigma2_hat*X.T.dot(X) + inv(S))\n",
    "    # mean of the posterior\n",
    "    MU = 1/sigma2_hat*SIGMA.dot(X.T).dot(t)\n",
    "    \n",
    "    #Compute predictions and uncertainty \n",
    "    pred_mean = X_new.dot(MU)\n",
    "    pred_var = (10 + X_new.dot(np.matrix(SIGMA).I).dot(X_new.T)).diagonal()\n",
    "        \n",
    "    return pred_mean, pred_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">b) Discuss how can you select the (hyper-)parameters for the Gaussian prior.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to select two parameters for our Gaussian prior : its mean vector and its covariance matrix Œ£. Since we do not know much prior knowledge, we should set its mean vector to 0 and its covariance to $ùúé^2ùêº$, with ùêº being the identity matrix and ùúé a sufficiently large positive value. We will set ùúé=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">c) Write code that calculates the N-th order polynomial transformation of the\n",
    "input data. For simplicity, do not consider polynomials of more than one variable\n",
    "(such as x2\n",
    "y), but raise each input variable to the power of N individually.\n",
    "Consider N=1, 2, 3, and 6. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_order(X,N):\n",
    "    if N==1 :\n",
    "        return np.column_stack([np.ones(X.shape[0]),X])\n",
    "    else :\n",
    "        X_n_1 = get_nth_order(X,N-1)\n",
    "        X_add = np.empty(X.shape)\n",
    "        for i in range(X.shape[0]):\n",
    "            for k in range(X.shape[1]):\n",
    "                X_add[i][k]=X[i][k]**N\n",
    "        return np.concatenate([X_n_1,X_add],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">\n",
    "d) Describe any additional pre-processing that you suggest for this data. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have a high number of features, we may want to perform feature selection, either with Principal Component Analysis (PCA) or a scoring method. We should also normalize the features before training our model to ensure numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardized(X):\n",
    "    res = np.zeros(X.shape)\n",
    "    for k in range(X.shape[1]) : \n",
    "        avg = np.mean(X[:,k])\n",
    "        std = np.std(X[:,k])\n",
    "        res[:,k] = (X[:,k]-avg)/std\n",
    "    return res\n",
    "\n",
    "X_train = standardized(trainDF.drop(['ID_code','target'],axis=1).values)\n",
    "y_train = trainDF['target'].values\n",
    "\n",
    "X_test = standardized(testDF.drop(['ID_code','target'],axis=1).values)\n",
    "y_test = testDF['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">e) Treat class labels as continuous and apply regression to the training data.\n",
    "Also, calculate the posterior variance of the weights.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply regression to the N-th order polynomial transformation input data for N in (1,2,3,6), and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29940\n",
      "Time taken for computing the predictions with 1-th order : 1.772 min\n",
      "Time taken for computing the predictions with 2-th order : 1.883 min\n",
      "Time taken for computing the predictions with 3-th order : 2.001 min\n",
      "Time taken for computing the predictions with 6-th order : 2.255 min\n"
     ]
    }
   ],
   "source": [
    "#Try with 30% of the training set to try avoiding memory error\n",
    "msk = np.random.rand(len(santanderDF)) < 0.3\n",
    "trainDF_half = santanderDF[msk]\n",
    "\n",
    "#Split 50%/50% between training and test\n",
    "msk = np.random.rand(len(trainDF_half)) < 0.5\n",
    "testDF_half = trainDF_half[~msk]\n",
    "trainDF_half = trainDF_half[msk]\n",
    "\n",
    "X_train = standardized(trainDF_half.drop(['ID_code','target'],axis=1).values)\n",
    "y_train = trainDF_half['target'].values\n",
    "\n",
    "X_test = standardized(testDF_half.drop(['ID_code','target'],axis=1).values)\n",
    "y_test = testDF_half['target'].values\n",
    "dictPerf = dict()\n",
    "print(len(X_train))\n",
    "for N in [1,2,3,6]:\n",
    "    time0 = time.time()\n",
    "    predictions, variances = bayes_lin_reg(X_train,X_test, y_train,N)\n",
    "    dictPerf[N]=(predictions,variances)\n",
    "    print('Time taken for computing the predictions with '+str(N)+'-th order : {:.3f} min'.format((time.time()-time0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">f) Suggest a way to discretize predictions and display the confusion matrix on the\n",
    "test data and report accuracy. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discretize our predictions by rounding to 0 if our prediction is lower than 0.5 and to 1 if our prediction is higher than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(predictions):\n",
    "    discretePreds = np.zeros(predictions.shape)\n",
    "    for i in range(len(predictions)) :\n",
    "        if predictions[i]>0.5 : \n",
    "            discretePreds[i] = 1\n",
    "    return discretePreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 0 :\n",
    "            if y_pred[i] == 0:\n",
    "                true_neg+=1\n",
    "        else : \n",
    "            false_pos+=1\n",
    "        if y_true[i] == 1 :\n",
    "            if y_pred[i] == 1:\n",
    "                true_pos+=1\n",
    "            else : \n",
    "                false_neg+=1    \n",
    "    return np.array([[true_pos, false_pos],[false_neg,true_neg]])\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    m = confusion_matrix(y_true,y_pred)\n",
    "    return (m[0][0]+m[1][1])/(m[0][0]+m[0][1]+m[1][0]+m[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  102  2948]\n",
      " [ 2846 26961]]\n",
      "Accuracy : 0.8236601028700125\n",
      "[[  102  2948]\n",
      " [ 2846 26961]]\n",
      "Accuracy : 0.8236601028700125\n",
      "[[  102  2948]\n",
      " [ 2846 26961]]\n",
      "Accuracy : 0.8236601028700125\n",
      "[[  102  2948]\n",
      " [ 2846 26961]]\n",
      "Accuracy : 0.8236601028700125\n"
     ]
    }
   ],
   "source": [
    "for N in [1,2,3,6]:\n",
    "    discretePreds = discretize(dictPerf[N][0])\n",
    "    print(confusion_matrix(y_test,discretePreds))\n",
    "    print('Accuracy : '+str(accuracy(y_test,discretePreds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">g) Discuss the performance, compare it against a classifier that outputs random class labels.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers trained with higher order polynomial transformation of the input data have exactly the same confusion matrix as the one trained with the original data. A classifier that outputs random labels would have an accuracy of around 0.5, so our classifier has a better performance. By looking at the confusion matrix, we see that our classifier predicts roughly the same ratio of positive samples than in the training data. It has a meager true positive rate of around 0.03 and a true negative rate of around 0.9. It seems that our classifier is biased toward the majority class.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">\n",
    "a) The goal is to implement a logistic regression classifier that optimizes for the\n",
    "Maximum a Posteriori (MAP) estimate; assume a Gaussian prior on the parameters.\n",
    "As a first step, write a function that calculates the gradient of the joint likelihood. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the value of w that maximize the posterior probability, that is :\n",
    "$$p(w|X, t, œÉ^2) = \\frac{p(t|X, w, œÉ^2)p(w|œÉ2)}{p(t|X, œÉ^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call this value $\\hat{w}$. Since they are proportional, $\\hat{w}$ also maximizes $p(t|X, w, œÉ^2)p(w|œÉ^2)$. \n",
    "Since $exp$ is an increasing function, this is equivalent to maximizing $log(p(t|X, w, œÉ^2)p(w|œÉ^2))$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume a centered Gaussian prior distribution of covariance $œÉ^2I$  : \n",
    "$p(w|œÉ^2) = {exp(-w^Tw/2œÉ)}.{(2\\pi)^\\frac{D}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assume that the samples are independent and equally distributed. That means that the likelihood respects : \n",
    "$$p(t|X, w, œÉ^2) = \\prod_{n=1}^N p(t_n|x_n, w))$$\n",
    "where $p(t_n=1|x_n, w)=\\frac{1}{1+exp(-w^Tx_n)}$ \n",
    "        and $p(t_n=0|x_n, w)=\\frac{exp(-w^Tx_n)}{1+exp(-w^Tx_n)}$.\n",
    "        \n",
    "We can note that $$p(t_n|x_n, w)=\\frac{1}{1+exp(-w^Tx_n)}^{t_n}.\\frac{exp(-w^Tx_n)}{1+exp(-w^Tx_n)}^{1-t_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define $h(w,X,t) = - log(ùëù(ùë°|ùëã,ùë§,œÉ^2)ùëù(ùë§|œÉ^2))$, the function we want to minimize.\n",
    "To do that, we first have to compute the gradient of the log-joint-likelihood.\n",
    "We will first rewrite the log-joint-likelihood :\n",
    "$$log(ùëù(ùë°|ùëã,ùë§,œÉ^2))=\\sum_{n=1}^N log(ùëù(ùë°ùëõ|ùë•ùëõ,ùë§))$$\n",
    "$$log(ùëù(ùë°|ùëã,ùë§,œÉ^2))= - \\sum_{n=1}^N {((1-t_n)w^Tx_n + log(1+exp(-w^Tx_n)))}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute its gradient :\n",
    "$\\frac{\\partial }{\\partial w}log(ùëù(ùë°|ùëã,ùë§,œÉ^2))=\\sum_{n=1}^N {x_n(t_n - \\frac{1}{1+exp(-w^Tx_n)})} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $h(w,X,t)$ over $w$ is then given by : $$ \\frac{\\partial h}{\\partial w}(w,X,t) = \\frac{1}{œÉ^2}w - \\sum_{n=1}^N {x_n(t_n - \\frac{1}{1+exp(-w^Tx_n)})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "def gradient_log_likelihood(w,X,t):\n",
    "    res=0\n",
    "    for n in range(X.shape[1]):\n",
    "        tmp = -w.T.dot(X[n])\n",
    "        if tmp < 0 : #Distinguish those cases to avoid calling exp with a value high enough to cause overflow\n",
    "            res+= (t[n]-1/(1+mt.exp(tmp)))*X[n]\n",
    "        else : \n",
    "            res+= (t[n]-(1-1/(1+mt.exp(-tmp))))*X[n]\n",
    "    return res\n",
    "\n",
    "def deriv_h(w,X,t):\n",
    "    return (1/sigma**2)*w - gradient_log_likelihood(w,X,t)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">b) Write a simple gradient descend algorithm that uses the gradients calculated\n",
    "by the function of previous question to converge to the MAP estimate. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(w0,X,t, min_improv, l_r,max_iter=1000):\n",
    "    n_iter=0\n",
    "    w_prev = w0\n",
    "    w_new = w_prev - (l_r*deriv_h(w_prev,X,t))\n",
    "    \n",
    "    # keep looping until improvement is smaller than stopping criteria\n",
    "    while sum((w_new-w_prev)**2) > min_improv and n_iter<max_iter:\n",
    "        n_iter+=1\n",
    "        # change the value of w\n",
    "        w_prev = w_new\n",
    "        \n",
    "        # get the derivation of the old value of w\n",
    "        d_h = deriv_h(w_prev,X,t)\n",
    "        \n",
    "        # get the new value of w by adding the previous, the multiplication of the derivative and the learning rate\n",
    "        w_new = w_prev - (l_r * d_h)\n",
    "    if n_iter == max_iter : print('Maximum iterations reached')\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate w with weights chosen uniformly between 0 and 1 \n",
    "w0 = np.empty(X_train.shape[1])\n",
    "for k in range(X_train.shape[1]):\n",
    "    w0[k] = np.random.uniform()\n",
    "\n",
    "#Use gradient descent to find local optimum\n",
    "for N in [1,2,3,6]:\n",
    "    X_train_nth = get_nth_order(X_train,N)\n",
    "    X_test_nth = get_nth_order(X_test,N)\n",
    "    #Initiate w with weights chosen uniformly between 0 and 1 \n",
    "    w0 = np.empty(X_train_nth.shape[1])\n",
    "    for k in range(X_train_nth.shape[1]):\n",
    "        w0[k] = np.random.uniform()\n",
    "    w_hat = step(w0, X_train_nth, y_train, 0.01, 0.001, max_iter=50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">c) Comment on the convexity of the problem; do you need multiple restarts in order to obtain a solution sufficiently close to the global optimum?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Show that w : h(w,X,t) is convex; Hessian? Sum of convex functions?)\n",
    "\n",
    "Since h is convex, if h reaches a local minimum on a value $\\hat{w}$, then $h(\\hat{w})$ will be a global minimum. This implies that if we set a sufficiently low learning rate and let it iterate a sufficient number of times, our gradient descent will not need multiple restarts to obtain an approximate solution with any precision we would like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">d) Report the confusion matrix and classification accuracy on the test data. Discuss logistic regression performance with respect to the performance of\n",
    "Bayesian linear regression.</p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_new,w):\n",
    "    tmp = -w.T.dot(x_new)\n",
    "    if tmp < 0 : #Distinguish those cases to avoid calling exp with a value high enough to cause overflow\n",
    "        prob = 1/(1+mt.exp(tmp))\n",
    "    else : \n",
    "        prob = 1-1/(1+mt.exp(-tmp))\n",
    "    if prob > 0.5 :\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  617  1952]\n",
      " [ 1335 15186]]\n",
      "Accuracy : 0.8278156102671556\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.empty(X_test_nth.shape[0])\n",
    "for n in range(X_test_nth.shape[0]):\n",
    "    y_pred[n] = predict(X_test_nth[n],w_hat)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy : '+str(accuracy(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier has a worse accuracy than the one we had with the Bayesian linear regression. It has a true positive rate of around 0.2 and a true negative rate of around 0.88. It seems that it is less biased towards the majority class than the previous classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue;\">e) Laplace approximation is an efficient way to obtain an approximate\n",
    "posterior for logistic regression. Describe the steps of this approach. What is the \n",
    "form of approximation obtained? [5] </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Laplace approximation, we want to approximate the posterior probability (that we can not compute) by a Gaussian $q(w|X, t) = N (¬µ, Œ£)$ that is easier to work with.\n",
    "The Gaussian we use to proximate the posterior has for mean vector the mode, that is the value $\\hat{w}$ of which we obtained numerically an approximation earlier. Its covariance matrix verifies : $$ Œ£ = - \\frac{\\partial ^2h}{\\partial {w} \\partial {w^T}}log(g(w,X,t)_{|\\hat{w}}$$\n",
    "Given $\\hat{w}$, we can compute exactly Œ£.\n",
    "This will give us better approximations for distributions that 'look' Gaussian.\n",
    "We can then draw samples from $N (¬µ, Œ£)$, use them to give a probability that a given instance belongs to a given class and average those probabilities to make our final prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
